{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0294c6c",
   "metadata": {},
   "source": [
    "# Evolutionary Strategies and Differential Evolution for Neural Network Optimization\n",
    "\n",
    "In this assignment, we will explore the application of **Evolutionary Strategies (ES) and Differential Evolution (DE)** to optimize the weights of a neural network for solving the **Lunar Lander problem**. The Lunar Lander environment, provided by OpenAI Gym, simulates the challenge of controlling a lunar module to land it safely on a designated landing pad. The task involves applying thrust and rotation to navigate the lander under the influence of gravity while avoiding crashes or drifting out of bounds.\n",
    "\n",
    "The goal is to train a neural network that acts as a controller for the lander. By optimizing the network weights, the controller learns how to achieve smooth and successful landings. Instead of traditional gradient-based optimization methods, we will use evolutionary algorithms, as introduced in class, to explore the weight space.\n",
    "\n",
    "In this context, evolutionary algorithms offer a robust and gradient-free optimization approach that is particularly useful for tasks with non-differentiable objectives or high-dimensional search spaces. You will program an ES/DE to optimize the network weights and compare its performance against a **Random Search** baseline, where weights are sampled randomly and evaluated.\n",
    "\n",
    "The problem at hand is to design an EA that finds a set of neural network weights maximizing the total reward in the Lunar Lander environment. Rewards are given for successful landings and penalized for crashes or inefficient maneuvers. By comparing the EA with Random Search, you will analyze the effectiveness and efficiency of evolutionary optimization methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e30b4",
   "metadata": {},
   "source": [
    "# Assignment Task\n",
    "\n",
    "In this assignment, you will use Evolutionary Strategies (ES) and Differential Evolution (DE) to optimize the weights of a neural network for the Lunar Lander environment from OpenAI Gym. You will compare the performance of your implementation with a **Random Search** baseline provided in the starter code.\n",
    "\n",
    "## 0. Setup\n",
    "Some modules are required to run the code:\n",
    "```python\n",
    "python -m pip install gymnasium torch swig\n",
    "python -m pip install \"gymnasium[box2d]\"\n",
    "```\n",
    "\n",
    "## 1. Analyze the provided implementation\n",
    "- Review the given **Random Search** code. Understand how it generates and evaluates random weight sets for the neural network. Run the code to familiarize yourself with its output and performance.\n",
    "\n",
    "## 2. Implement the following:\n",
    "- **2.1 Choose one variant of ES** covered in class, such as CMA-ES, NES.\n",
    "- **2.2 Implement the selected ES** in Python. Make sure to:\n",
    "  - Represent the weights of the neural network as the individuals in the population.\n",
    "  - Design a fitness function based on the reward obtained in the Lunar Lander environment.\n",
    "  - Include key algorithm mechanisms.\n",
    "\n",
    "## 3. Run experiments and compare:\n",
    "Conduct experiments with both the **Random Search** and your EA implementation. For each experiment, run **5 different random seeds** to account for variability. Analyze the following setup:\n",
    "- **3.1 Your ES implementation vs. Random Search.**\n",
    "\n",
    "### Metrics to compare:\n",
    "- **3.3 Best reward achieved**: Compare the highest rewards obtained by each setup.\n",
    "- **3.4 Convergence speed**: Measure the number of generations required to reach a reward threshold (e.g., a landing reward of 200 or higher).\n",
    "- **3.5 Consistency**: Report the average and standard deviation of the results across the random seeds.\n",
    "\n",
    "## 4. Analyze the results:\n",
    "Report the findings of your experiments in a structured format. Your analysis should include:\n",
    "- **4.1 Best reward achieved for each setup** (include plots of reward vs. generation for each method).\n",
    "- **4.2 Average and standard deviation of rewards across runs.**\n",
    "- **4.3 Convergence speed (generations to reach the threshold) across different setups.**\n",
    "- **4.4 Observations on the performance improvements (if any) from the enhancements you implemented.**\n",
    "\n",
    "## 5. Report the best-performing setup:\n",
    "Summarize the best setup you found for optimizing the Lunar Lander environment, justifying your choice based on your experimental results.\n",
    "\n",
    "---\n",
    "\n",
    "### Note:\n",
    "- As in previous assignments, ensure you run the same setup multiple times with different random seeds to draw reliable conclusions.\n",
    "- Use the theoretical slides and material discussed in class to support your analysis.\n",
    "\n",
    "### Further Reading:\n",
    "- [OpenAI Gym Documentation](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "- Additional material shared in class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9fb54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb95bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4),\n",
    "            nn.Tanh(),\n",
    "            #nn.Linear(4, 4),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(4, output_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return torch.argmax(output).item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9d9b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LunarLanderAgent:\n",
    "    def __init__(self, env_name='LunarLander-v3', search_algorithm=None, episodes=1, time_steps=1000):\n",
    "        self.env = gym.make(\"LunarLander-v3\", continuous=False, enable_wind=False)\n",
    "        self.episodes = episodes\n",
    "        self.time_steps = time_steps\n",
    "        self.search_algorithm = search_algorithm\n",
    "        self.input_dim = self.env.observation_space.shape[0]\n",
    "        self.output_dim = self.env.action_space.n  # Discrete action space\n",
    "        self.mlp = self.create_mlp(self.input_dim, self.output_dim)\n",
    "    \n",
    "    def create_mlp(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Creates an MLP using PyTorch.\n",
    "        \"\"\"\n",
    "        mlp = MLP(input_dim, output_dim)\n",
    "        return mlp\n",
    "    \n",
    "    def get_param_vector(self):\n",
    "        params = []\n",
    "        for param in self.mlp.parameters():\n",
    "            params.append(param.data.cpu().numpy().flatten())\n",
    "        return np.concatenate(params)\n",
    "    \n",
    "    def set_param_vector(self, vector):\n",
    "        offset = 0\n",
    "        for param in self.mlp.parameters():\n",
    "            shape = param.shape\n",
    "            size = np.prod(shape)\n",
    "            param.data = torch.tensor(vector[offset:offset + size].reshape(shape), dtype=torch.float32)\n",
    "            offset += size\n",
    "    \n",
    "    def evaluate(self, vector):\n",
    "        self.set_param_vector(vector)\n",
    "        total_reward = 0\n",
    "        for _ in range(self.episodes):\n",
    "            obs = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            terminated = False\n",
    "            obs = obs[0]\n",
    "            i = 0\n",
    "            while not terminated and i < self.time_steps:\n",
    "                obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "                action = self.mlp(obs)\n",
    "                #print(action)\n",
    "                obs, reward, terminated, done, info = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                i += 1\n",
    "            total_reward += episode_reward\n",
    "        #print(total_reward)\n",
    "        return total_reward / self.episodes\n",
    "\n",
    "    def train(self):\n",
    "        if self.search_algorithm:\n",
    "            self.best_params = self.search_algorithm(self)\n",
    "            self.save_agent(\"best_agent.pkl\")\n",
    "        else:\n",
    "            raise ValueError(\"No search algorithm provided\")\n",
    "        \n",
    "    def test(self):\n",
    "        env = gym.make('LunarLander-v3', render_mode=\"human\")\n",
    "        obs = env.reset()\n",
    "        total_reward = 0\n",
    "        terminated = False\n",
    "        obs = obs[0]\n",
    "        i = 0\n",
    "        while not terminated and i < self.time_steps:\n",
    "            env.render()\n",
    "            obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "            action = self.mlp(obs)\n",
    "            obs, reward, terminated, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            i += 1\n",
    "        print(f\"Test Reward: {total_reward}\")\n",
    "        env.close()\n",
    "        \n",
    "\n",
    "    def save_agent(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.get_param_vector(), f)\n",
    "\n",
    "    def load_agent(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            param_vector = pickle.load(f)\n",
    "        self.set_param_vector(param_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe25a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_evolution_plot(best, mean, title, save=False):\n",
    "    plt.plot(best, label='Best Reward')\n",
    "    plt.plot(mean, label='Mean Reward')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.draw()\n",
    "    if save:\n",
    "        plt.savefig(f'{title}.png')\n",
    "    plt.pause(0.01)\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "364e51ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_search(agent, population_size=200, generations=20, alpha=0.01, sigma=0.1):\n",
    "    \"\"\"\n",
    "    Optimize the MLP using Random Search.\n",
    "    \"\"\"\n",
    "    param_vector = agent.get_param_vector()\n",
    "    num_params = len(param_vector)\n",
    "    best_params = param_vector\n",
    "    best_reward = -np.inf\n",
    "    best_rewards = []\n",
    "    mean_rewards = []\n",
    "    for generation in range(generations):\n",
    "        population = []\n",
    "        for i in range(population_size):\n",
    "            ind = param_vector + sigma * np.random.randn(num_params)\n",
    "            population.append(ind)\n",
    "        rewards = []\n",
    "        for individual in population:\n",
    "            r = agent.evaluate(individual)\n",
    "            rewards.append(r)\n",
    "        rewards = np.array(rewards)\n",
    "        max_reward_idx = np.argmax(rewards)\n",
    "        if rewards[max_reward_idx] > best_reward:\n",
    "            best_reward = rewards[max_reward_idx]\n",
    "            best_params = population[max_reward_idx]\n",
    "            agent.save_agent(f'best_agent_random_search.pkl')\n",
    "        # Logging\n",
    "        print(f\"Generation {generation + 1}: Best Reward = {rewards.max()} Mean Reward = {rewards.mean()}\")\n",
    "\n",
    "        best_rewards.append(rewards.max())\n",
    "        mean_rewards.append(rewards.mean())\n",
    "    make_evolution_plot(best_rewards, mean_rewards, \"RS\", True)\n",
    "    agent.set_param_vector(best_params)\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "136a79ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_search(agent, alpha=10, beta=40, generations=20, sigma=0.1, alpha_step=0.01):\n",
    "    \"\"\"\n",
    "    Evolution Strategy (α + β) with elitist selection.\n",
    "    \"\"\"\n",
    "    # Initialize alpha parents\n",
    "    parents = [agent.get_param_vector() + sigma * np.random.randn(*agent.get_param_vector().shape) for _ in range(alpha)]\n",
    "    \n",
    "    for generation in range(generations):\n",
    "        # Generate beta offspring from alpha parents\n",
    "        offspring = []\n",
    "        for _ in range(beta):\n",
    "            parent_idx = np.random.randint(0, alpha)  # Randomly choose a parent\n",
    "            parent = parents[parent_idx]\n",
    "            mutation = sigma * np.random.randn(*parent.shape)\n",
    "            offspring.append(parent + mutation)\n",
    "        \n",
    "        # Evaluate fitness for parents and offspring\n",
    "        population = parents + offspring\n",
    "        fitness_scores = np.array([agent.evaluate(ind) for ind in population])\n",
    "        \n",
    "        # Select the top α individuals for the next generation (elitist selection)\n",
    "        elite_indices = np.argsort(fitness_scores)[-alpha:]  # Top alpha individuals\n",
    "        parents = [population[i] for i in elite_indices]\n",
    "        \n",
    "        # Track the best solution\n",
    "        best_params = parents[-1]\n",
    "        best_reward = fitness_scores[elite_indices[-1]]\n",
    "        agent.set_param_vector(best_params)\n",
    "        \n",
    "        # Save the best model\n",
    "        agent.save_agent(f'best_agent_es_alpha_beta.pkl')\n",
    "        \n",
    "        # Logging\n",
    "        print(f\"Generation {generation + 1}: Best Reward = {best_reward}, Mean Reward = {fitness_scores.mean()}\")\n",
    "\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187984da",
   "metadata": {},
   "source": [
    "## Train !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6790da7",
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Box2D is not installed, you can install it by run `pip install swig` followed by `pip install \"gymnasium[box2d]\"`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\PC\\Desktop\\MECD\\CE\\.tp4\\lib\\site-packages\\gymnasium\\envs\\box2d\\bipedal_walker.py:15\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBox2D\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m         circleShape,\n\u001b[0;32m     18\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         revoluteJointDef,\n\u001b[0;32m     23\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m46\u001b[39m)\n\u001b[0;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m46\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mLunarLanderAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_algorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_search\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m, in \u001b[0;36mLunarLanderAgent.__init__\u001b[1;34m(self, env_name, search_algorithm, episodes, time_steps)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLunarLander-v3\u001b[39m\u001b[38;5;124m'\u001b[39m, search_algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, time_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLunarLander-v3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontinuous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_wind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisodes \u001b[38;5;241m=\u001b[39m episodes\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_steps \u001b[38;5;241m=\u001b[39m time_steps\n",
      "File \u001b[1;32mc:\\Users\\PC\\Desktop\\MECD\\CE\\.tp4\\lib\\site-packages\\gymnasium\\envs\\registration.py:704\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    701\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m env_spec\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[1;32m--> 704\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload_env_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;66;03m# Determine if to use the rendering\u001b[39;00m\n\u001b[0;32m    707\u001b[0m render_modes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\Desktop\\MECD\\CE\\.tp4\\lib\\site-packages\\gymnasium\\envs\\registration.py:551\u001b[0m, in \u001b[0;36mload_env_creator\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    The environment constructor for the given environment name.\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    550\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 551\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\PC\\Desktop\\MECD\\CE\\.tp4\\lib\\site-packages\\gymnasium\\envs\\box2d\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbipedal_walker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcar_racing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CarRacing\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlunar_lander\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LunarLander, LunarLanderContinuous\n",
      "File \u001b[1;32mc:\\Users\\PC\\Desktop\\MECD\\CE\\.tp4\\lib\\site-packages\\gymnasium\\envs\\box2d\\bipedal_walker.py:25\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m         circleShape,\n\u001b[0;32m     18\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         revoluteJointDef,\n\u001b[0;32m     23\u001b[0m     )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBox2D is not installed, you can install it by run `pip install swig` followed by `pip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgymnasium[box2d]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     27\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpygame\u001b[39;00m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: Box2D is not installed, you can install it by run `pip install swig` followed by `pip install \"gymnasium[box2d]\"`"
     ]
    }
   ],
   "source": [
    "# Evolve and Train!\n",
    "np.random.seed(46)\n",
    "torch.random.manual_seed(46)\n",
    "agent = LunarLanderAgent(search_algorithm=random_search, episodes=3, time_steps=1000)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f0887",
   "metadata": {},
   "source": [
    "## Test ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd1ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test!\n",
    "agent = LunarLanderAgent(search_algorithm=random_search, episodes=3, time_steps=1000)\n",
    "agent.load_agent(f'best_agent_random_search.pkl')\n",
    "agent.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598a580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf27e895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
